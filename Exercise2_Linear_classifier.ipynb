{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilarakhimova/hse-cvt/blob/main/Exercise2_Linear_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRBhMZtdf94"
      },
      "source": [
        "# Download dataset with Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNE5Qqw_rdsV"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKAZ-aQtrg8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfe21c8-802e-4042-90be-f64dce7b48a9"
      },
      "source": [
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "GENERATOR = torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "# Define transformation for each image\n",
        "transform  = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x).flatten()) #Stretch image into row [32,32,3] -> [3072]\n",
        "      ])\n",
        "\n",
        "# Download a CIFAR10 dataset\n",
        "dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=True,\n",
        "                           transform = transform,\n",
        "                           download=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3F4IoDErlcw"
      },
      "source": [
        "## Split dataset & define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu02DmABYxoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95cf01f3-daee-4da9-9fcc-ecf7c906f66d"
      },
      "source": [
        "train_ds, val_ds, _ = random_split(dataset, [20000, 1000, 29000], GENERATOR)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size)\n",
        "\n",
        "# Test Dataloader\n",
        "for images, class_nums in train_loader:\n",
        "  print(images.shape, class_nums.shape) # class_nums are tensor!\n",
        "  pil_img = Image.fromarray(images[0].reshape(32, 32, 3).numpy())\n",
        "  display(pil_img,\n",
        "          class_nums[0].item()\n",
        "          )\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 3072]) torch.Size([256])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqC6xXhSeUPr"
      },
      "source": [
        "# Implement LinearClassifier class for CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaxIJUM7eQUp"
      },
      "source": [
        "class LinearClassifier:\n",
        "  def __init__(self, labels, ):\n",
        "    self.labels = labels\n",
        "    self.classes_num = len(labels)\n",
        "    self.W = np.random.randn(3072, self.classes_num) * 0.0001\n",
        "    self.batch_size = 200\n",
        "\n",
        "  def train(self, x_batch, y_batch, learning_rate = 1e-8):\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        x  (numpy.array): collection of objects (batch)\n",
        "        y  (numpy.array): collection of integer\n",
        "        representing a class number for objects from x\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    loss_val, grad = self.loss(x_batch, y_batch)\n",
        "\n",
        "    # update weights\n",
        "    self.W -= learning_rate * grad\n",
        "\n",
        "    return loss_val / x_batch.shape[0]\n",
        "\n",
        "  def loss(self, x, y):    # x and y are batches\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        x  (numpy.array): collection of objects (batch)\n",
        "        y  (numpy.array): collection of integer\n",
        "        representing a class number for objects from x\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(self.W.shape)\n",
        "\n",
        "    # calculate Multiclass SVM loss over a batch\n",
        "    delta = 1.0\n",
        "    scores = x.dot(self.W)\n",
        "    y_scores = scores[np.arange(x.shape[0]), y]\n",
        "    margins = np.maximum(0, scores - y_scores[:, np.newaxis] + delta)\n",
        "    margins[np.arange(x.shape[0]), y] = 0\n",
        "    loss = np.sum(margins)\n",
        "\n",
        "    # calculate gradients (dL/dW) and store it in dW\n",
        "    binary = margins\n",
        "    binary[margins > 0] = 1\n",
        "    row_sum = np.sum(binary, axis=1)\n",
        "    binary[np.arange(x.shape[0]), y] = -row_sum.T\n",
        "    dW = np.dot(x.T, binary)\n",
        "\n",
        "    dW /= x.shape[0]\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "  def predict(self,x):\n",
        "    scores = x.dot(self.W)\n",
        "    return np.argmax(scores,axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyVPgrr5xjhU"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5zVN1kHd43W"
      },
      "source": [
        "## Function for accuracy checking\n",
        "\n",
        "Don't change this code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhRClCsdzJw"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate(model, dataloader):\n",
        "  y_predicted = np.array([])\n",
        "  y_gtrue = np.array([])\n",
        "  for images, class_nums in dataloader:\n",
        "    index = model.predict(images.numpy())\n",
        "    y_predicted = np.append(y_predicted, index)\n",
        "    y_gtrue = np.append(y_gtrue, class_nums.numpy())\n",
        "  return accuracy_score(y_gtrue, y_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQr1qYUlxq7X"
      },
      "source": [
        "## Train loop\n",
        "Let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phcDEj7OdpGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8e8342-fc18-4eca-d6f6-65765d2dcdd9"
      },
      "source": [
        "model = LinearClassifier(dataset.classes)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images.numpy(), class_nums.numpy())\n",
        "    accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 6.335427094895989, Accuracy: 0.191\n",
            "Epoch 1 Loss: 5.106434270682216, Accuracy: 0.219\n",
            "Epoch 2 Loss: 4.483723152078512, Accuracy: 0.241\n",
            "Epoch 3 Loss: 4.110670588891439, Accuracy: 0.263\n",
            "Epoch 4 Loss: 3.852096054425542, Accuracy: 0.286\n",
            "Epoch 5 Loss: 3.6589269236956863, Accuracy: 0.302\n",
            "Epoch 6 Loss: 3.50975009955449, Accuracy: 0.308\n",
            "Epoch 7 Loss: 3.3894617362326454, Accuracy: 0.309\n",
            "Epoch 8 Loss: 3.2918774168508778, Accuracy: 0.319\n",
            "Epoch 9 Loss: 3.208145115314566, Accuracy: 0.319\n",
            "Epoch 10 Loss: 3.1367132367327994, Accuracy: 0.323\n",
            "Epoch 11 Loss: 3.0638529505146614, Accuracy: 0.326\n",
            "Epoch 12 Loss: 3.000599023514957, Accuracy: 0.335\n",
            "Epoch 13 Loss: 2.9455941014561864, Accuracy: 0.336\n",
            "Epoch 14 Loss: 2.8910021040821747, Accuracy: 0.34\n",
            "Epoch 15 Loss: 2.8365123358754936, Accuracy: 0.342\n",
            "Epoch 16 Loss: 2.7854898827819055, Accuracy: 0.346\n",
            "Epoch 17 Loss: 2.740490761763814, Accuracy: 0.349\n",
            "Epoch 18 Loss: 2.700938440084457, Accuracy: 0.35\n",
            "Epoch 19 Loss: 2.6618598636965, Accuracy: 0.351\n",
            "Epoch 20 Loss: 2.626984824909677, Accuracy: 0.352\n",
            "Epoch 21 Loss: 2.592126226999236, Accuracy: 0.352\n",
            "Epoch 22 Loss: 2.5588398319828616, Accuracy: 0.357\n",
            "Epoch 23 Loss: 2.529362547767774, Accuracy: 0.361\n",
            "Epoch 24 Loss: 2.5030397355998057, Accuracy: 0.366\n",
            "Best accuracy is 0.366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4tIFR5bwZFi"
      },
      "source": [
        "# Check model on test dataset\n",
        "\n",
        "You must get accuracy above 0.35\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM0pWYJlwibm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97719292-2103-4aba-b960-dcc523815010"
      },
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transform, # Transforms stay the same\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = len(test_dataset))\n",
        "\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test: {accuracy}, rounding to 2 decimal places we get accuracy: {round(accuracy, 2)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test: 0.3489, rounding to 2 decimal places we get accuracy: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsImxpggG8bH"
      },
      "source": [
        "# Place for brief conclusion\n",
        "\n",
        "Сразу бросается в глаза кардинальное улучшение результатов метрики `accuracy` в сравнении с её результатами в прошлой домашней работе, при использовании алгоритма `kNN`. Также гораздо меньше требуется вычислительных мощностей и памяти: `линейный классификатор` работает значительно быстрее, так как не требует прохождения много раз циклом по обучающей выборке, да и в целом, после обработки данных и получения значений весов, обучающая выборка более не нужна.\n",
        "\n",
        "\n",
        "В ходе моих экспериментов выяснилось, что лучше всего линейный классификатор предсказывает классы изображений из датасета `CIFAR10` с функцией потерь SVM и регуляризатором, судя по данным метрики `accuracy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ13OmfCEb1w"
      },
      "source": [
        "# Ideas for extra work\n",
        "\n",
        "- Implement CrossEntropyLoss function✅\n",
        "- Add regularization to SVM loss✅\n",
        "- Normalize data✅\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Cross Entropy loss function"
      ],
      "metadata": {
        "id": "-YazfVCgRVuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform  = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x).flatten())\n",
        "      ])\n",
        "\n",
        "dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=True,\n",
        "                           transform = transform,\n",
        "                           download=True)\n",
        "\n",
        "train_ds, val_ds, _ = random_split(dataset, [20000, 1000, 29000], GENERATOR)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iz_etjag8b4",
        "outputId": "757563cf-dc12-484b-cfca-a235e892aa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearClassifierCE:\n",
        "  def __init__(self, labels, ):\n",
        "    self.labels = labels\n",
        "    self.classes_num = len(labels)\n",
        "    self.W = np.random.randn(3072, self.classes_num) * 0.0001\n",
        "    self.batch_size = 200\n",
        "\n",
        "  def train(self, x_batch, y_batch, learning_rate = 1e-8):\n",
        "    loss_val, grad = self.loss(x_batch, y_batch)\n",
        "\n",
        "    self.W -= learning_rate * grad\n",
        "\n",
        "    return loss_val / x_batch.shape[0]\n",
        "\n",
        "  def loss(self, x, y):\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(self.W.shape)\n",
        "\n",
        "    # calculate Cross Enthropy loss over a batch\n",
        "    scores = x.dot(self.W)\n",
        "    scores -= np.max(scores, axis=1)[:, np.newaxis]\n",
        "    p = np.exp(scores) / np.sum(np.exp(scores), axis=1)[:, np.newaxis]\n",
        "\n",
        "    loss -= np.sum(np.log(p[np.arange(x.shape[0]), y]))\n",
        "\n",
        "    # calculate gradients (dL/dW) and store it in dW\n",
        "    probs = p\n",
        "    probs[np.arange(x.shape[0]), y] -= 1\n",
        "\n",
        "    dW = x.T.dot(probs)\n",
        "\n",
        "    dW /= x.shape[0]\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "  def predict(self,x):\n",
        "    scores = x.dot(self.W)\n",
        "    return np.argmax(scores,axis = 1)"
      ],
      "metadata": {
        "id": "agXLyGnWRU_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearClassifierCE(dataset.classes)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images.numpy(), class_nums.numpy())\n",
        "    accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glCgqAIDhHB6",
        "outputId": "e17af439-0b89-4bc4-b4e1-1331a98b2d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 2.340380909387698, Accuracy: 0.132\n",
            "Epoch 1 Loss: 2.2974845972756626, Accuracy: 0.16\n",
            "Epoch 2 Loss: 2.262221916432066, Accuracy: 0.172\n",
            "Epoch 3 Loss: 2.232302242821664, Accuracy: 0.182\n",
            "Epoch 4 Loss: 2.2066698777163385, Accuracy: 0.197\n",
            "Epoch 5 Loss: 2.1845077359389222, Accuracy: 0.21\n",
            "Epoch 6 Loss: 2.1651774294943915, Accuracy: 0.218\n",
            "Epoch 7 Loss: 2.1481779175029554, Accuracy: 0.236\n",
            "Epoch 8 Loss: 2.1331131983726666, Accuracy: 0.245\n",
            "Epoch 9 Loss: 2.119667707233236, Accuracy: 0.251\n",
            "Epoch 10 Loss: 2.1075878024852583, Accuracy: 0.255\n",
            "Epoch 11 Loss: 2.0966678688691838, Accuracy: 0.263\n",
            "Epoch 12 Loss: 2.0867398544322233, Accuracy: 0.268\n",
            "Epoch 13 Loss: 2.0776653443400392, Accuracy: 0.272\n",
            "Epoch 14 Loss: 2.0693295092746347, Accuracy: 0.275\n",
            "Epoch 15 Loss: 2.0616364451504507, Accuracy: 0.277\n",
            "Epoch 16 Loss: 2.0545055525098177, Accuracy: 0.283\n",
            "Epoch 17 Loss: 2.047868699147497, Accuracy: 0.29\n",
            "Epoch 18 Loss: 2.0416679779220823, Accuracy: 0.293\n",
            "Epoch 19 Loss: 2.0358539208723987, Accuracy: 0.297\n",
            "Epoch 20 Loss: 2.0303840662181543, Accuracy: 0.297\n",
            "Epoch 21 Loss: 2.0252218005592897, Accuracy: 0.299\n",
            "Epoch 22 Loss: 2.0203354174051436, Accuracy: 0.302\n",
            "Epoch 23 Loss: 2.015697347036309, Accuracy: 0.302\n",
            "Epoch 24 Loss: 2.011283523016576, Accuracy: 0.302\n",
            "Best accuracy is 0.302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transform,\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = len(test_dataset))\n",
        "\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test: {accuracy}, rounding to 2 decimal places we get accuracy: {round(accuracy, 2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf1So2T8hKB5",
        "outputId": "dd6f9d1a-f798-4941-d2c7-dc29fa0272f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test: 0.3012, rounding to 2 decimal places we get accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add regularization to SVM loss (the squared L2 norm)"
      ],
      "metadata": {
        "id": "1C4CgnYCNevB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearClassifierWithReg:\n",
        "  def __init__(self, labels, ):\n",
        "    self.labels = labels\n",
        "    self.classes_num = len(labels)\n",
        "    self.W = np.random.randn(3072, self.classes_num) * 0.0001\n",
        "    self.batch_size = 200\n",
        "\n",
        "  def train(self, x_batch, y_batch, learning_rate = 1e-8, reg=1e-6):\n",
        "    loss_val, grad = self.loss(x_batch, y_batch, reg)\n",
        "\n",
        "    # update weights\n",
        "    self.W -= learning_rate * grad\n",
        "\n",
        "    return loss_val / x_batch.shape[0] + reg * np.sum(self.W * self.W)\n",
        "\n",
        "  def loss(self, x, y, reg):    # x and y are batches\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(self.W.shape)\n",
        "\n",
        "    # calculate Multiclass SVM loss over a batch (with regularization)\n",
        "    delta = 1.0\n",
        "    scores = x.dot(self.W)\n",
        "    y_scores = scores[np.arange(x.shape[0]), y]\n",
        "    margins = np.maximum(0, scores - y_scores[:, np.newaxis] + delta)\n",
        "    margins[np.arange(x.shape[0]), y] = 0\n",
        "    loss = np.sum(margins)\n",
        "\n",
        "    # calculate gradients (dL/dW) and store it in dW\n",
        "    binary = margins\n",
        "    binary[margins > 0] = 1\n",
        "    row_sum = np.sum(binary, axis=1)\n",
        "    binary[np.arange(x.shape[0]), y] = -row_sum.T\n",
        "    dW = np.dot(x.T, binary)\n",
        "\n",
        "    dW /= x.shape[0]\n",
        "    dW += 2 * reg * self.W\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "  def predict(self,x):\n",
        "    scores = x.dot(self.W)\n",
        "    return np.argmax(scores,axis = 1)"
      ],
      "metadata": {
        "id": "xMNFvGBwNl9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearClassifierWithReg(dataset.classes)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images.numpy(), class_nums.numpy())\n",
        "    accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laxq9FnSO-Ne",
        "outputId": "16c4f05b-6ecf-4c3c-dc86-034690a55545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.626464941762967, Accuracy: 0.188\n",
            "Epoch 1 Loss: 4.814485828749676, Accuracy: 0.236\n",
            "Epoch 2 Loss: 4.361232655491445, Accuracy: 0.258\n",
            "Epoch 3 Loss: 4.062898897422364, Accuracy: 0.262\n",
            "Epoch 4 Loss: 3.8217498746757594, Accuracy: 0.278\n",
            "Epoch 5 Loss: 3.6247071016803565, Accuracy: 0.291\n",
            "Epoch 6 Loss: 3.478712820714233, Accuracy: 0.302\n",
            "Epoch 7 Loss: 3.366909762382134, Accuracy: 0.317\n",
            "Epoch 8 Loss: 3.280350422360774, Accuracy: 0.32\n",
            "Epoch 9 Loss: 3.210025582961491, Accuracy: 0.322\n",
            "Epoch 10 Loss: 3.1424415022389183, Accuracy: 0.324\n",
            "Epoch 11 Loss: 3.0766555151213417, Accuracy: 0.327\n",
            "Epoch 12 Loss: 3.0150425132092726, Accuracy: 0.327\n",
            "Epoch 13 Loss: 2.960088708599476, Accuracy: 0.33\n",
            "Epoch 14 Loss: 2.911855665266179, Accuracy: 0.331\n",
            "Epoch 15 Loss: 2.868684451587943, Accuracy: 0.341\n",
            "Epoch 16 Loss: 2.829985457366628, Accuracy: 0.34\n",
            "Epoch 17 Loss: 2.789680553400929, Accuracy: 0.339\n",
            "Epoch 18 Loss: 2.7529783017253027, Accuracy: 0.342\n",
            "Epoch 19 Loss: 2.723179115683981, Accuracy: 0.343\n",
            "Epoch 20 Loss: 2.690172061474113, Accuracy: 0.343\n",
            "Epoch 21 Loss: 2.660464198330754, Accuracy: 0.343\n",
            "Epoch 22 Loss: 2.6242358376804233, Accuracy: 0.346\n",
            "Epoch 23 Loss: 2.5945693852872425, Accuracy: 0.348\n",
            "Epoch 24 Loss: 2.562947043484953, Accuracy: 0.35\n",
            "Best accuracy is 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transform, # Transforms stay the same\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = len(test_dataset))\n",
        "\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test: {accuracy}, rounding to 2 decimal places we get accuracy: {round(accuracy, 2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd8jqCazPeOu",
        "outputId": "7d77483a-979f-4403-cdb2-6a802c2368ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test: 0.3502, rounding to 2 decimal places we get accuracy: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize data"
      ],
      "metadata": {
        "id": "2uIKofxnRbYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Finding mean and std for train part of a dataset*"
      ],
      "metadata": {
        "id": "Lofx7sQ9fdFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size = 200)\n",
        "\n",
        "imgs = torch.stack([img_t for img_t, _ in train_loader], dim=1)\n",
        "mean, std = imgs.view(3, -1).mean(dim=1), imgs.view(3, -1).std(dim=1)\n",
        "print(f\"mean: {mean}, std: {std}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjs9mJTOdJp7",
        "outputId": "efd3fd33-6f29-4904-b623-684c05335b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: tensor([0.4718, 0.4728, 0.4734]), std: tensor([0.2512, 0.2512, 0.2504])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformNorm  = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "      transforms.Normalize(mean, std),\n",
        "        transforms.Lambda(lambda x: np.array(x).flatten()),\n",
        "          ])\n",
        "\n",
        "dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=True,\n",
        "                           transform = transformNorm,\n",
        "                           download=True)\n",
        "\n",
        "train_ds, val_ds, _ = random_split(dataset, [20000, 1000, 29000], GENERATOR)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRG7Qqb7e2uj",
        "outputId": "d125cb99-1900-414e-9014-7ed3f3351171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearClassifier(dataset.classes)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images.numpy(), class_nums.numpy())\n",
        "    accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyvxZymZe8bt",
        "outputId": "94d28d9b-37fa-4923-f174-12b447b49cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 9.004444429807226, Accuracy: 0.108\n",
            "Epoch 1 Loss: 9.001548600221051, Accuracy: 0.114\n",
            "Epoch 2 Loss: 8.998652770634878, Accuracy: 0.119\n",
            "Epoch 3 Loss: 8.995756941048704, Accuracy: 0.122\n",
            "Epoch 4 Loss: 8.992861111462531, Accuracy: 0.127\n",
            "Epoch 5 Loss: 8.989965281876357, Accuracy: 0.129\n",
            "Epoch 6 Loss: 8.987069452290182, Accuracy: 0.132\n",
            "Epoch 7 Loss: 8.98417362270401, Accuracy: 0.135\n",
            "Epoch 8 Loss: 8.981277793117835, Accuracy: 0.14\n",
            "Epoch 9 Loss: 8.97838196353166, Accuracy: 0.147\n",
            "Epoch 10 Loss: 8.975486133945488, Accuracy: 0.148\n",
            "Epoch 11 Loss: 8.972590304359313, Accuracy: 0.151\n",
            "Epoch 12 Loss: 8.969694474773139, Accuracy: 0.15\n",
            "Epoch 13 Loss: 8.966798645186966, Accuracy: 0.154\n",
            "Epoch 14 Loss: 8.963902815600791, Accuracy: 0.159\n",
            "Epoch 15 Loss: 8.961006986014617, Accuracy: 0.165\n",
            "Epoch 16 Loss: 8.958111156428444, Accuracy: 0.167\n",
            "Epoch 17 Loss: 8.95521532684227, Accuracy: 0.168\n",
            "Epoch 18 Loss: 8.952319497256095, Accuracy: 0.176\n",
            "Epoch 19 Loss: 8.949423667669922, Accuracy: 0.177\n",
            "Epoch 20 Loss: 8.946527838083748, Accuracy: 0.176\n",
            "Epoch 21 Loss: 8.943632008497573, Accuracy: 0.182\n",
            "Epoch 22 Loss: 8.9407361789114, Accuracy: 0.18\n",
            "Epoch 23 Loss: 8.937840349325226, Accuracy: 0.186\n",
            "Epoch 24 Loss: 8.934944519739052, Accuracy: 0.191\n",
            "Best accuracy is 0.191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transformNorm,\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = len(test_dataset))\n",
        "\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test: {accuracy}, rounding to 2 decimal places we get accuracy: {round(accuracy, 2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehz0p1CLcp_K",
        "outputId": "1a010afc-03a7-4895-88c3-3066e9bf3360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test: 0.174, rounding to 2 decimal places we get accuracy: 0.17\n"
          ]
        }
      ]
    }
  ]
}